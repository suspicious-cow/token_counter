# Token Counter - Enhanced LLM Comparison Tool

A system for comparing token usage, costs, and outputs across multiple Large Language Model (LLM) providers with analytics and monitoring capabilities.

## üöÄ New Enhanced Features

### Architecture Improvements

- **Modular Client Architecture**: Base client with standardized interfaces
- **Client Factory Pattern**: Centralized client management and validation
- **Enhanced Error Handling**: Retry logic with exponential backoff
- **Rate Limiting**: Rate limiting to prevent API throttling
- **Analytics**: Cost analysis and visualization

### Enhanced User Experience

- **Interactive CLI Mode**: Guided experiment setup
- **Rich Console Output**: Terminal interface with progress indicators
- **Reporting**: Analysis with charts and statistics
- **API Key Validation**: Automatic validation and status reporting

### Advanced Analytics

- **Cost Comparison Charts**: Visual cost analysis across providers
- **Token Efficiency Analysis**: Output tokens per dollar calculations
- **Outlier Detection**: Identify unusual responses or costs
- **Success Rate Tracking**: Monitor API reliability by provider

## üöÄ How to Use the System

### Prerequisites

1. **Install dependencies**:

   ```bash
   pip install -r requirements.txt
   ```

2. **Set up API keys** as environment variables:

   ```bash

   export OPENAI_API_KEY="your-openai-api-key"
   export ANTHROPIC_API_KEY="your-anthropic-api-key"
   export GEMINI_API_KEY="your-gemini-api-key"
   export GROK_API_KEY="your-grok-api-key"
   ```

### Usage Methods

#### 1. Interactive Mode (Recommended for Beginners)

```bash
python cli/interactive.py
```

**What it does**: Guides you through experiment setup, validates API keys, and allows provider selection.

#### 2. Command Line Interface

```bash
python main.py [OPTIONS]
```

### üìã Command Line Parameters

#### Core Parameters

| Parameter  | Short | Type    | Default                      | Description                            |
| ---------- | ----- | ------- | ---------------------------- | -------------------------------------- |
| `--prompt` | `-p`  | string  | "Give me the word 'halt'..." | The user prompt to send to all LLMs    |
| `--system` | `-s`  | string  | ""                           | System instructions for the LLMs       |
| `--trials` | `-t`  | integer | 3                            | Number of times to run each experiment |
| `--output` | `-o`  | string  | auto-generated               | Output CSV filename                    |

#### Provider Selection

| Parameter   | Short | Type   | Default | Description                               |
| ----------- | ----- | ------ | ------- | ----------------------------------------- |
| `--vendors` | `-v`  | string | all     | Comma-separated list of providers to test |

**Valid providers**: `openai`, `gemini`, `anthropic`, `grok`

#### Feature Flags

| Parameter         | Type | Description                                                      |
| ----------------- | ---- | ---------------------------------------------------------------- |
| `--enhanced`      | flag | Enable advanced features (rate limiting, retry logic, analytics) |
| `--validate-only` | flag | Only validate API keys and exit (no experiments)                 |

### üìö Detailed Usage Examples

#### Basic Usage

```bash
# Simple experiment with default settings
python main.py

# Custom prompt with 5 trials
python main.py --prompt "Explain quantum computing in one sentence" --trials 5

# Test specific providers only
python main.py --vendors openai,gemini --trials 3
```

#### Enhanced Mode (Recommended)

```bash
# Enhanced mode with all features
python main.py --enhanced --prompt "Write a haiku about AI" --trials 5

# Enhanced mode with specific providers and custom output
python main.py --enhanced --vendors openai,anthropic --trials 10 --output "ai_haiku_test.csv"

# Enhanced mode with system prompt
python main.py --enhanced --prompt "Hello" --system "Be very concise" --trials 3
```

#### Validation and Setup

```bash
# Check which API keys are properly configured
python main.py --validate-only

# Demo the features without making API calls
python examples/demo.py
```

### üîß Parameter Details and Examples

#### `--prompt` / `-p`

**Purpose**: The main text you want to send to all LLM providers for comparison.

```bash
# Short prompt
python main.py --prompt "Hello world"

# Complex prompt
python main.py --prompt "Write a Python function that calculates fibonacci numbers"

# Prompt with quotes
python main.py --prompt "Explain the concept of 'machine learning' to a 5-year-old"
```

#### `--system` / `-s`

**Purpose**: System-level instructions that guide the LLM's behavior and response style.

```bash
# Concise responses
python main.py --system "Be very brief and direct" --prompt "What is AI?"

# Specific format
python main.py --system "Respond only in JSON format" --prompt "List 3 colors"

# Role-playing
python main.py --system "You are a helpful coding assistant" --prompt "Fix this Python code"
```

#### `--trials` / `-t`

**Purpose**: Number of times to repeat the experiment with each provider for statistical analysis.

```bash
# Single trial (fastest)
python main.py --trials 1 --prompt "Test"

# Multiple trials for statistical significance
python main.py --trials 10 --prompt "Generate a random number"

# High trial count for research
python main.py --trials 20 --enhanced --prompt "Creative writing sample"
```

**Recommendation**: Use 3-5 trials for general testing, 10+ for research or when response variability is important.

#### `--vendors` / `-v`

**Purpose**: Select specific LLM providers to test instead of all available ones.

```bash
# Test only OpenAI and Gemini
python main.py --vendors openai,gemini

# Test only Anthropic
python main.py --vendors anthropic --trials 5

# Test all except Grok
python main.py --vendors openai,gemini,anthropic
```

**Use cases**:

- Cost control (test cheaper providers first)
- Speed (fewer providers = faster results)
- Comparison studies (specific provider comparisons)

#### `--output` / `-o`

**Purpose**: Specify custom filename for results instead of auto-generated timestamped files.

```bash
# Custom filename
python main.py --output "my_experiment.csv"

# Organized by date
python main.py --output "results/2024-01-15-test.csv"

# Descriptive naming
python main.py --output "gpt_vs_claude_coding_test.csv"
```

**Default behavior**: Auto-generates timestamped files like `api_raw_20240115_143022_CST.csv`

#### `--enhanced`

**Purpose**: Enables advanced features for production use.

**What it adds**:

- Rate limiting to prevent API throttling
- Retry logic with exponential backoff
- Analytics and visualizations
- Error handling
- Cost comparison charts
- Token efficiency analysis

```bash
# Basic enhanced mode
python main.py --enhanced

# Enhanced with custom settings
python main.py --enhanced --trials 10 --vendors openai,anthropic
```

**When to use**: Always recommended except for quick tests or when dependencies aren't available.

#### `--validate-only`

**Purpose**: Check API key configuration without running experiments.

```bash
# Check all API keys
python main.py --validate-only
```

**Output example**:

```
API Key Validation Report:
------------------------------
Openai       ‚úÖ Valid
Gemini       ‚úÖ Valid
Anthropic    ‚ùå Invalid/Missing
Grok         ‚úÖ Valid

Valid providers: 3/4
```

### üí° Best Practices

#### For Beginners

```bash
# Start with validation
python main.py --validate-only

# Try interactive mode
python cli/interactive.py

# Simple test
python main.py --enhanced --trials 1 --prompt "Hello"
```

#### For Research/Analysis

```bash
# Comprehensive comparison
python main.py --enhanced --trials 10 --prompt "Your research prompt here"

# Cost-focused analysis
python main.py --enhanced --vendors gemini,openai --trials 5 --prompt "Efficiency test"
```

#### For Development/Testing

```bash
# Quick iteration
python main.py --trials 1 --vendors openai --prompt "Test prompt"

# Specific provider testing
python main.py --enhanced --vendors anthropic --trials 3
```

## üß† Reasoning Model Handling

This system is configured to avoid reasoning tokens that could skew token count comparisons:

### Model Selection

- **OpenAI**: `gpt-4o` (non-reasoning model)
- **Gemini**: `gemini-2.5-flash` with reasoning **disabled** (`thinking_budget=0`)
- **Anthropic**: `claude-3-5-sonnet-20241022` (non-reasoning model)
- **Grok**: `grok-beta` (non-reasoning model)

### Reasoning Configuration

- **Gemini 2.5 Flash**: Controllable reasoning via `thinking_budget` parameter
  - Set to `0` to disable internal reasoning (behaves like non-reasoning models)
  - Configurable in `config.py`: `GEMINI_THINKING_BUDGET = 0`
- **Other providers**: Use standard non-reasoning models

This ensures fair token count comparisons without hidden reasoning overhead that could affect cost analysis.

## üí∞ Pricing Configuration

### Centralized Pricing Management

All pricing is configured in `config.py` under the `MODELS_INFO` dictionary - **not in individual client files**. This provides:

- **Single source of truth** for all provider pricing
- **Easy maintenance** - update prices in one location
- **Consistent cost calculations** across all experiments

### Current Pricing (USD per 1M tokens)

_Last updated: January 2025_

| Provider  | Model                      | Input | Cached Input     | Output |
| --------- | -------------------------- | ----- | ---------------- | ------ |
| OpenAI    | gpt-4o                     | $2.50 | $1.25 (50% off)  | $10.00 |
| Gemini    | gemini-2.5-flash           | $0.30 | $0.075 (75% off) | $2.50  |
| Anthropic | claude-3-5-sonnet-20241022 | $3.00 | $0.30 (90% off)  | $15.00 |
| Grok      | grok-beta                  | $5.00 | $1.25 (75% off)  | $15.00 |

### Caching Details by Provider

#### OpenAI (gpt-4o)

- **Discount**: 50% off cached input tokens
- **Minimum**: 1024+ tokens required for caching
- **Type**: Automatic prompt caching for repeated prefixes
- **Management**: No explicit cache control needed

#### Gemini (gemini-2.5-flash)

- **Discount**: 75% off cached input tokens
- **Minimum**: 32K+ tokens for explicit caching
- **Type**: Both implicit (automatic) and explicit context caching
- **Management**: Implicit caching occurs automatically

#### Anthropic (claude-3-5-sonnet-20241022)

- **Discount**: 90% off cache reads, 25% premium on cache writes
- **Minimum**: 1024+ tokens required for caching
- **Type**: Explicit prompt caching with cache control blocks
- **Management**: Cache expires ~5 minutes after last access

#### Grok (grok-beta)

- **Discount**: 75% off cached input tokens
- **Minimum**: No documented minimum (may cache very short prompts)
- **Type**: Automatic prompt caching (OpenAI-compatible)
- **Management**: Automatic for repeated prompt prefixes

### Updating Pricing

To update pricing when providers change rates:

1. **Edit `config.py`**:

   ```python
   MODELS_INFO = {
       "openai": {
           "input_cost_per_million": 2.50,    # Update this value
           "cached_input_cost_per_million": 1.25,  # Update this value
           "output_cost_per_million": 10.00   # Update this value
       },
       # ... other providers
   }
   ```

2. **Update the "Last updated" comment** in `config.py`

3. **No client code changes needed** - pricing is automatically applied

### Cost Calculation Method

The system calculates costs using official API token counts:

```
uncached_input_cost = (total_input_tokens - cached_tokens) √ó input_rate
cached_input_cost = cached_tokens √ó cached_input_rate
output_cost = output_tokens √ó output_rate
total_cost = uncached_input_cost + cached_input_cost + output_cost
```

## üìä Enhanced Output Files

Each experiment now generates:

- **Raw Data CSV**: Complete results with token counts and costs
- **Analysis Report**: Statistical summary with efficiency metrics
- **Cost Comparison Charts**: Visual analysis (PNG format)
- **Failed Calls Log**: Detailed error tracking for troubleshooting

## üèóÔ∏è Architecture Overview

### New File Structure

```
‚îú‚îÄ‚îÄ clients/                   # Enhanced client architecture
‚îÇ   ‚îú‚îÄ‚îÄ base_client.py        # Abstract base client
‚îÇ   ‚îú‚îÄ‚îÄ openai_client.py      # Enhanced OpenAI client
‚îÇ   ‚îú‚îÄ‚îÄ gemini_client.py      # Enhanced Gemini client
‚îÇ   ‚îú‚îÄ‚îÄ anthropic_client.py   # Enhanced Anthropic client
‚îÇ   ‚îî‚îÄ‚îÄ grok_client.py        # Enhanced Grok client
‚îú‚îÄ‚îÄ config/                   # Configuration management
‚îÇ   ‚îî‚îÄ‚îÄ validation.py         # API key validation utilities
‚îú‚îÄ‚îÄ utils/                    # Utility modules
‚îÇ   ‚îú‚îÄ‚îÄ retry.py              # Retry logic with backoff
‚îÇ   ‚îî‚îÄ‚îÄ rate_limiter.py       # Rate limiting utilities
‚îú‚îÄ‚îÄ analytics/                # Advanced analytics
‚îÇ   ‚îî‚îÄ‚îÄ analyzer.py           # Comprehensive analysis tools
‚îú‚îÄ‚îÄ cli/                      # Interactive CLI
‚îÇ   ‚îî‚îÄ‚îÄ interactive.py        # Guided experiment setup
‚îú‚îÄ‚îÄ tests/                    # Test suite
‚îÇ   ‚îî‚îÄ‚îÄ test_clients.py       # Client testing
‚îú‚îÄ‚îÄ examples/                 # Demo and example scripts
‚îÇ   ‚îî‚îÄ‚îÄ demo.py               # Feature demonstration
‚îú‚îÄ‚îÄ client_factory.py         # Client factory pattern
‚îú‚îÄ‚îÄ main.py                   # Main script (classic + enhanced modes)
‚îî‚îÄ‚îÄ [other files...]          # Configuration, requirements, etc.
```

### Client Architecture

All clients now inherit from `BaseLLMClient` providing:

- Standardized response format (`LLMResponse` dataclass)
- Consistent error handling
- Unified cost calculations
- Token usage tracking (`TokenUsage` dataclass)

```python
from client_factory import ClientFactory

# Get any client
client = ClientFactory.get_client('openai')
response = client.process("Hello world")

print(f"Output: {response.output}")
print(f"Cost: ${response.cost}")
print(f"Tokens: {response.usage.input_tokens} in, {response.usage.output_tokens} out")
```

## üîß Enhanced Configuration

### API Key Validation

```python
from config.validation import validate_api_keys, print_validation_report

# Check all API keys
validation_results = validate_api_keys()
print_validation_report()
```

### Rate Limiting Configuration

```python
from utils.rate_limiter import RateLimiter

# Custom rate limits
limiter = RateLimiter({
    'openai': 0.1,    # 10 calls/second
    'gemini': 1.0,    # 1 call/second
})
```

## üìà Advanced Analytics

### Comprehensive Analysis

```python
from analytics.analyzer import ExperimentAnalyzer

analyzer = ExperimentAnalyzer(results_df)

# Generate cost comparison charts
analyzer.generate_cost_comparison_chart()

# Calculate token efficiency
efficiency = analyzer.generate_token_efficiency_report()

# Detect outliers
outliers = analyzer.detect_outliers()

# Generate full report
analyzer.generate_comprehensive_report()
```

### Sample Analytics Output

```
Token Efficiency (Output Tokens per Dollar):
  Gemini: 83,333 tokens/$
  OpenAI: 12,500 tokens/$
  Grok: 6,667 tokens/$
  Anthropic: 3,333 tokens/$

Outlier Detection:
  High cost outliers: 2
  High token outliers: 1
  Unusual responses: 0
```

## üß™ Testing

### Run Test Suite

```bash
# Install test dependencies
pip install pytest

# Run all tests
python -m pytest tests/

# Run specific test file
python -m pytest tests/test_clients.py -v
```

### Test Coverage

- Client architecture validation
- Cost calculation accuracy
- Error handling scenarios
- Provider validation
- Token counting precision

## üö¶ Error Handling & Resilience

### Retry Logic

- Exponential backoff with jitter
- Configurable retry attempts
- Smart error classification

### Rate Limiting

- Provider-specific limits
- Automatic throttling
- Configurable intervals

### Comprehensive Error Reporting

- Detailed failed call logs
- Success rate tracking by provider
- Error categorization and analysis

## üé® Interactive Mode Features

The new interactive CLI provides:

- **Guided Setup**: Step-by-step experiment configuration
- **API Key Validation**: Real-time status checking
- **Provider Selection**: Choose from available providers
- **Rich Interface**: Terminal output with tables and panels
- **Confirmation Steps**: Review before execution

```bash
python cli/interactive.py
```

## üìä Enhanced Reporting

### Experiment Summary

- Success rates by provider
- Cost analysis with statistics
- Token efficiency rankings
- Outlier identification

### Visual Analytics

- Cost distribution box plots
- Token usage comparisons
- Efficiency bar charts
- Success rate visualizations

## üîÑ Backward Compatibility

All original functionality is preserved:

- Original `main.py` works unchanged
- All client functions maintain same signatures
- Existing scripts continue to work
- Configuration remains compatible

## üöÄ Performance Improvements

### Concurrent Processing

- Rate-limited concurrent API calls
- Intelligent request batching
- Optimized retry strategies

### Memory Efficiency

- Streaming result processing
- Efficient data structures
- Minimal memory footprint

## üõ†Ô∏è Development Features

### Extensibility

- Easy to add new providers
- Plugin architecture ready
- Configurable components

### Monitoring

- Comprehensive logging
- Performance metrics
- Error tracking

### Testing

- Unit test coverage
- Integration tests
- Mock API responses

## üìã Migration Guide

### From Original to Enhanced

1. **Install new dependencies**:

   ```bash
   pip install -r requirements.txt
   ```

2. **Try enhanced mode**:

   ```bash
   python main.py --validate-only
   ```

3. **Use interactive mode**:

   ```bash
   python cli/interactive.py
   ```

4. **Existing scripts work unchanged** - no migration required!

## ü§ù Contributing

The enhanced architecture makes contributions easier:

- Clear separation of concerns
- Comprehensive test suite
- Standardized interfaces
- Detailed documentation

## üìÑ License

This project is for educational and research purposes. Please respect each API provider's terms of service.

---

**Enhanced Token Counter** - Production-ready LLM comparison with advanced analytics and monitoring capabilities.
