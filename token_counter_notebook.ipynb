{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "39284a43",
   "metadata": {},
   "source": [
    "# Token Counter Notebook\n",
    "\n",
    "This notebook allows you to input text, retrieve outputs, and calculate token counts using OpenAI, Gemini, Anthropic, and Grok APIs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "294f2b80",
   "metadata": {},
   "source": [
    "## 1. Install and Import Required Libraries\n",
    "\n",
    "Install necessary libraries such as `openai`, `anthropic`, and others. Import required modules like `os` and `json`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e441ae2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required libraries (uncomment if running for the first time)\n",
    "# !pip install openai anthropic google-generativeai requests pandas\n",
    "\n",
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import openai\n",
    "import anthropic\n",
    "from google import genai\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "311df612",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make output cells take full width\n",
    "from IPython.core.display import HTML\n",
    "HTML(\"<style>.jp-OutputArea-output { max-width: 100vw !important; }</style>\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83c8ca06",
   "metadata": {},
   "source": [
    "## 2. Set Up API Keys\n",
    "\n",
    "Set up environment variables or directly define API keys for OpenAI, Gemini, Anthropic, and Grok."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "638367d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set your API keys here or load them from environment variables\n",
    "\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\", \"your-openai-api-key\")\n",
    "ANTHROPIC_API_KEY = os.getenv(\"ANTHROPIC_API_KEY\", \"your-anthropic-api-key\")\n",
    "GEMINI_API_KEY = os.getenv(\"GEMINI_API_KEY\", \"your-gemini-api-key\")\n",
    "GROK_API_KEY = os.getenv(\"GROK_API_KEY\", \"your-grok-api-key\")\n",
    "\n",
    "# Set API keys for respective libraries if possible\n",
    "openai.api_key = OPENAI_API_KEY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc3a11ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enter your user prompt here\n",
    "prompt = 'Say the word hello.'\n",
    "\n",
    "# Enter your system prompt here\n",
    "system_prompt = ''\n",
    "\n",
    "# Enter the number of trials to run\n",
    "num_trials = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df72b691",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_with_openai_chat(prompt, system_prompt, model=\"gpt-4o\"):\n",
    "    try:\n",
    "        from openai import OpenAI\n",
    "        client = OpenAI(api_key=OPENAI_API_KEY)\n",
    "        \n",
    "        messages = []\n",
    "        if system_prompt:  # Only add system message if not empty\n",
    "            messages.append({\"role\": \"system\", \"content\": system_prompt})\n",
    "        messages.append({\"role\": \"user\", \"content\": prompt})\n",
    "        \n",
    "        response = client.chat.completions.create(\n",
    "            model=model,\n",
    "            messages=messages\n",
    "        )\n",
    "        output = response.choices[0].message.content\n",
    "        input_tokens = getattr(response.usage, 'prompt_tokens', None)\n",
    "        output_tokens = getattr(response.usage, 'completion_tokens', None)\n",
    "        return output, input_tokens, output_tokens\n",
    "    except Exception as e:\n",
    "        return f\"OpenAI ChatCompletions error: {str(e)}\", None, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bab4cccc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_with_gemini(prompt, system_prompt, model=\"gemini-2.0-flash\"):\n",
    "    try:\n",
    "        from google import genai\n",
    "        from google.genai import types\n",
    "        client = genai.Client(api_key=GEMINI_API_KEY)\n",
    "        \n",
    "        # Create config with system instruction only if not empty\n",
    "        config = None\n",
    "        if system_prompt:\n",
    "            config = types.GenerateContentConfig(system_instruction=system_prompt)\n",
    "        else:\n",
    "            config = types.GenerateContentConfig()\n",
    "            \n",
    "        response = client.models.generate_content(\n",
    "            model=model,\n",
    "            config=config,\n",
    "            contents=prompt\n",
    "        )\n",
    "        output = getattr(response, 'text', str(response))\n",
    "        usage = getattr(response, 'usage_metadata', None)\n",
    "        input_tokens = usage.prompt_token_count if usage and hasattr(usage, 'prompt_token_count') else None\n",
    "        output_tokens = usage.candidates_token_count if usage and hasattr(usage, 'candidates_token_count') else None\n",
    "        return output, input_tokens, output_tokens\n",
    "    except Exception as e:\n",
    "        return f\"Gemini error: {str(e)}\", None, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb5d98dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_with_anthropic(prompt, system_prompt, model=\"claude-3-7-sonnet-20250219\"):\n",
    "    try:\n",
    "        import anthropic\n",
    "        client = anthropic.Anthropic(api_key=ANTHROPIC_API_KEY)\n",
    "        \n",
    "        # Create message with system prompt only if not empty\n",
    "        kwargs = {\n",
    "            \"model\": model,\n",
    "            \"max_tokens\": 1024,\n",
    "            \"messages\": [{\"role\": \"user\", \"content\": prompt}]\n",
    "        }\n",
    "        if system_prompt:  # Only add system if not empty\n",
    "            kwargs[\"system\"] = system_prompt\n",
    "            \n",
    "        message = client.messages.create(**kwargs)\n",
    "        output = message.content[0].text if hasattr(message.content[0], 'text') else str(message.content[0])\n",
    "        input_tokens = getattr(message.usage, 'input_tokens', None)\n",
    "        output_tokens = getattr(message.usage, 'output_tokens', None)\n",
    "        return output, input_tokens, output_tokens\n",
    "    except Exception as e:\n",
    "        return f\"Anthropic error: {str(e)}\", None, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "664cd833",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_with_grok(prompt, system_prompt, model=\"grok-3-beta\"):\n",
    "    try:\n",
    "        from openai import OpenAI\n",
    "        client = OpenAI(\n",
    "            api_key=GROK_API_KEY,\n",
    "            base_url=\"https://api.x.ai/v1\",\n",
    "        )\n",
    "        \n",
    "        messages = []\n",
    "        if system_prompt:  # Only add system message if not empty\n",
    "            messages.append({\"role\": \"system\", \"content\": system_prompt})\n",
    "        messages.append({\"role\": \"user\", \"content\": prompt})\n",
    "        \n",
    "        completion = client.chat.completions.create(\n",
    "            model=model,\n",
    "            messages=messages\n",
    "        )\n",
    "        output = completion.choices[0].message.content\n",
    "        input_tokens = getattr(completion.usage, 'prompt_tokens', None)\n",
    "        output_tokens = getattr(completion.usage, 'completion_tokens', None)\n",
    "        return output, input_tokens, output_tokens\n",
    "    except Exception as e:\n",
    "        return f\"Grok error: {str(e)}\", None, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72c008eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the experiments for the specified number of trials\n",
    "all_rows = []\n",
    "\n",
    "for run in range(1, num_trials + 1):\n",
    "    print(f\"Running trial {run}/{num_trials}...\")\n",
    "    results = {}\n",
    "    \n",
    "    # Test OpenAI\n",
    "    try:\n",
    "        output, in_tok, out_tok = process_with_openai_chat(prompt, system_prompt)\n",
    "        results['OpenAI'] = {'output': output, 'input_tokens': in_tok, 'output_tokens': out_tok, 'model': 'gpt-4o'}\n",
    "    except Exception as e:\n",
    "        results['OpenAI'] = {'output': str(e), 'input_tokens': None, 'output_tokens': None, 'model': 'gpt-4o'}\n",
    "    \n",
    "    # Test Gemini\n",
    "    try:\n",
    "        output, in_tok, out_tok = process_with_gemini(prompt, system_prompt)\n",
    "        results['Gemini'] = {'output': output, 'input_tokens': in_tok, 'output_tokens': out_tok, 'model': 'gemini-2.0-flash'}\n",
    "    except Exception as e:\n",
    "        results['Gemini'] = {'output': str(e), 'input_tokens': None, 'output_tokens': None, 'model': 'gemini-2.0-flash'}\n",
    "    \n",
    "    # Test Anthropic\n",
    "    try:\n",
    "        output, in_tok, out_tok = process_with_anthropic(prompt, system_prompt)\n",
    "        results['Anthropic'] = {'output': output, 'input_tokens': in_tok, 'output_tokens': out_tok, 'model': 'claude-3-7-sonnet-20250219'}\n",
    "    except Exception as e:\n",
    "        results['Anthropic'] = {'output': str(e), 'input_tokens': None, 'output_tokens': None, 'model': 'claude-3-7-sonnet-20250219'}\n",
    "    \n",
    "    # Test Grok\n",
    "    try:\n",
    "        output, in_tok, out_tok = process_with_grok(prompt, system_prompt)\n",
    "        results['Grok'] = {'output': output, 'input_tokens': in_tok, 'output_tokens': out_tok, 'model': 'grok-3-beta'}\n",
    "    except Exception as e:\n",
    "        results['Grok'] = {'output': str(e), 'input_tokens': None, 'output_tokens': None, 'model': 'grok-3-beta'}\n",
    "    \n",
    "    # Add results to the master list\n",
    "    for vendor, res in results.items():\n",
    "        all_rows.append({\n",
    "            'Run Number': run,\n",
    "            'Vendor': vendor,\n",
    "            'Model': res['model'],\n",
    "            'User Prompt': prompt,\n",
    "            'System Prompt': system_prompt,\n",
    "            'Output': res['output'],\n",
    "            'Input Tokens': res['input_tokens'],\n",
    "            'Output Tokens': res['output_tokens']\n",
    "        })\n",
    "\n",
    "# Create DataFrame and display results\n",
    "df = pd.DataFrame(all_rows)\n",
    "print(f\"\\nCompleted {num_trials} trials with {len(df)} total API calls.\")\n",
    "print(\"\\nResults:\")\n",
    "display(df)\n",
    "\n",
    "# Output all results to a CSV file\n",
    "csv_path = 'api_results.csv'\n",
    "df.to_csv(csv_path, index=False, encoding='utf-8')\n",
    "print(f'\\nResults saved to {csv_path}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "all_llms",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
