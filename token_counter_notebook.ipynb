{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "39284a43",
   "metadata": {},
   "source": [
    "# Token Counter Notebook\n",
    "\n",
    "This notebook allows you to input text, retrieve outputs, and calculate token counts using OpenAI, Gemini, Anthropic, and Grok APIs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "294f2b80",
   "metadata": {},
   "source": [
    "## 1. Install and Import Required Libraries\n",
    "\n",
    "Install necessary libraries such as `openai`, `anthropic`, and others. Import required modules like `os` and `json`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8e441ae2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required libraries (uncomment if running for the first time)\n",
    "# !pip install openai anthropic google-generativeai requests tiktoken\n",
    "\n",
    "import os\n",
    "import json\n",
    "import openai\n",
    "import anthropic\n",
    "import google.generativeai as genai\n",
    "import requests\n",
    "import tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "311df612",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.jp-OutputArea-output { max-width: 100vw !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Make output cells take full width\n",
    "from IPython.core.display import HTML\n",
    "HTML(\"<style>.jp-OutputArea-output { max-width: 100vw !important; }</style>\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83c8ca06",
   "metadata": {},
   "source": [
    "## 2. Set Up API Keys\n",
    "\n",
    "Set up environment variables or directly define API keys for OpenAI, Gemini, Anthropic, and Grok."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "638367d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set your API keys here or load them from environment variables\n",
    "\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\", \"your-openai-api-key\")\n",
    "ANTHROPIC_API_KEY = os.getenv(\"ANTHROPIC_API_KEY\", \"your-anthropic-api-key\")\n",
    "GEMINI_API_KEY = os.getenv(\"GEMINI_API_KEY\", \"your-gemini-api-key\")\n",
    "GROK_API_KEY = os.getenv(\"GROK_API_KEY\", \"your-grok-api-key\")\n",
    "\n",
    "# Set API keys for respective libraries\n",
    "openai.api_key = OPENAI_API_KEY\n",
    "genai.configure(api_key=GEMINI_API_KEY)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f1b2486",
   "metadata": {},
   "source": [
    "## 3. Define Helper Functions for Token Counting\n",
    "\n",
    "Write helper functions to calculate token counts for each API, ensuring compatibility with their respective tokenization methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1af6c4d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_openai_tokens(text, model=\"gpt-3.5-turbo\"):\n",
    "    enc = tiktoken.encoding_for_model(model)\n",
    "    return len(enc.encode(text))\n",
    "\n",
    "def count_gemini_tokens(text):\n",
    "    # Gemini uses SentencePiece tokenizer, but for simplicity, use whitespace split as a proxy\n",
    "    return len(text.split())\n",
    "\n",
    "def count_anthropic_tokens(text):\n",
    "    # Anthropic uses Claude tokenizer, which is similar to GPT-3's\n",
    "    enc = tiktoken.get_encoding(\"cl100k_base\")\n",
    "    return len(enc.encode(text))\n",
    "\n",
    "def count_grok_tokens(text):\n",
    "    # Grok's tokenizer is not public; use whitespace split as a proxy\n",
    "    return len(text.split())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea36191c",
   "metadata": {},
   "source": [
    "## 4. Process Text with OpenAI API\n",
    "\n",
    "Send the input text to the OpenAI API, retrieve the output, and calculate the token count using the helper function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3aba21f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_with_openai(prompt, model=\"gpt-4o\"):\n",
    "    try:\n",
    "        import openai\n",
    "        client = openai.OpenAI(api_key=OPENAI_API_KEY)\n",
    "        response = client.chat.completions.create(\n",
    "            model=model,\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "                {\"role\": \"user\", \"content\": prompt}\n",
    "            ]\n",
    "        )\n",
    "        output = response.choices[0].message.content\n",
    "        input_tokens = response.usage.prompt_tokens if hasattr(response, 'usage') else None\n",
    "        output_tokens = response.usage.completion_tokens if hasattr(response, 'usage') else None\n",
    "        return output, input_tokens, output_tokens\n",
    "    except Exception as e:\n",
    "        return f\"OpenAI error: {str(e)}\", None, None\n",
    "\n",
    "# Example usage:\n",
    "# prompt = \"Explain the theory of relativity.\"\n",
    "# output, input_tokens, output_tokens = process_with_openai(prompt)\n",
    "# print(f\"Output: {output}\\nInput tokens: {input_tokens}\\nOutput tokens: {output_tokens}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4374ab37",
   "metadata": {},
   "source": [
    "## 5. Process Text with Gemini API\n",
    "\n",
    "Send the input text to the Gemini API, retrieve the output, and calculate the token count using the helper function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bab4cccc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_with_gemini(prompt, model=\"gemini-2.0-flash\"):\n",
    "    try:\n",
    "        from google import genai\n",
    "        client = genai.Client(api_key=GEMINI_API_KEY)\n",
    "        response = client.models.generate_content(\n",
    "            model=model,\n",
    "            contents=[prompt]\n",
    "        )\n",
    "        output = getattr(response, 'text', str(response))\n",
    "        # Token usage may not be available; set to None\n",
    "        return output, None, None\n",
    "    except Exception as e:\n",
    "        return f\"Gemini error: {str(e)}\", None, None\n",
    "\n",
    "# Example usage:\n",
    "# output, input_tokens, output_tokens = process_with_gemini(prompt)\n",
    "# print(f\"Output: {output}\\nInput tokens: {input_tokens}\\nOutput tokens: {output_tokens}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89c02840",
   "metadata": {},
   "source": [
    "## 6. Process Text with Anthropic API\n",
    "\n",
    "Send the input text to the Anthropic API, retrieve the output, and calculate the token count using the helper function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb5d98dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_with_anthropic(prompt, model=\"claude-3-opus-20240229\"):\n",
    "    try:\n",
    "        from claude_sdk import Claude\n",
    "        client = Claude(api_key=ANTHROPIC_API_KEY)\n",
    "        response = client.generate(\n",
    "            model=model,\n",
    "            prompt=prompt,\n",
    "            max_tokens=1000\n",
    "        )\n",
    "        output = str(response)\n",
    "        # Token usage may be available in response, else None\n",
    "        return output, None, None\n",
    "    except Exception as e:\n",
    "        return f\"Anthropic error: {str(e)}\", None, None\n",
    "\n",
    "# Example usage:\n",
    "# output, input_tokens, output_tokens = process_with_anthropic(prompt)\n",
    "# print(f\"Output: {output}\\nInput tokens: {input_tokens}\\nOutput tokens: {output_tokens}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da12f0c7",
   "metadata": {},
   "source": [
    "## 7. Process Text with Grok API\n",
    "\n",
    "Send the input text to the Grok API, retrieve the output, and calculate the token count using the helper function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "664cd833",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_with_grok(prompt, model=\"grok-3\"):\n",
    "    try:\n",
    "        import requests\n",
    "        url = \"https://api.x.ai/v1/grok3/completions\"\n",
    "        headers = {\"Authorization\": f\"Bearer {GROK_API_KEY}\"}\n",
    "        data = {\n",
    "            \"model\": model,\n",
    "            \"messages\": [{\"role\": \"user\", \"content\": prompt}]\n",
    "        }\n",
    "        response = requests.post(url, headers=headers, json=data, timeout=15)\n",
    "        if response.status_code == 200:\n",
    "            result = response.json()\n",
    "            output = result['choices'][0]['message']['content']\n",
    "            # Token usage may be in result['usage']\n",
    "            input_tokens = result.get('usage', {}).get('prompt_tokens')\n",
    "            output_tokens = result.get('usage', {}).get('completion_tokens')\n",
    "        else:\n",
    "            output = f\"Grok API error: {response.status_code} - {response.text}\"\n",
    "            input_tokens = None\n",
    "            output_tokens = None\n",
    "        return output, input_tokens, output_tokens\n",
    "    except Exception as e:\n",
    "        return f\"Grok error: {str(e)}\", None, None\n",
    "\n",
    "# Example usage:\n",
    "# output, input_tokens, output_tokens = process_with_grok(prompt)\n",
    "# print(f\"Output: {output}\\nInput tokens: {input_tokens}\\nOutput tokens: {output_tokens}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16b09ee1",
   "metadata": {},
   "source": [
    "## 8. Enter Your Prompt\n",
    "\n",
    "Define your input text in the variable below. This prompt will be sent to all four APIs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c1af8226",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enter your prompt here\n",
    "prompt = 'Explain the theory of relativity in simple terms.'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c9323d1",
   "metadata": {},
   "source": [
    "## 9. Run All APIs and Compare Results\n",
    "\n",
    "This cell will send your prompt to OpenAI, Gemini, Anthropic, and Grok, then display their outputs and token counts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "72c008eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>output</th>\n",
       "      <th>input_tokens</th>\n",
       "      <th>output_tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>OpenAI</th>\n",
       "      <td>OpenAI error: \\n\\nYou tried to access openai.C...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Gemini</th>\n",
       "      <td>Gemini error: 404 models/gemini-pro is not fou...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Anthropic</th>\n",
       "      <td>The theory of relativity, developed by Albert ...</td>\n",
       "      <td>11.0</td>\n",
       "      <td>278.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Grok</th>\n",
       "      <td>Grok error: HTTPSConnectionPool(host='api.grok...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                      output  input_tokens  \\\n",
       "OpenAI     OpenAI error: \\n\\nYou tried to access openai.C...           NaN   \n",
       "Gemini     Gemini error: 404 models/gemini-pro is not fou...           NaN   \n",
       "Anthropic  The theory of relativity, developed by Albert ...          11.0   \n",
       "Grok       Grok error: HTTPSConnectionPool(host='api.grok...           NaN   \n",
       "\n",
       "           output_tokens  \n",
       "OpenAI               NaN  \n",
       "Gemini               NaN  \n",
       "Anthropic          278.0  \n",
       "Grok                 NaN  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "results = {}\n",
    "# OpenAI\n",
    "try:\n",
    "    output, in_tok, out_tok = process_with_openai(prompt)\n",
    "    results['OpenAI'] = {'output': output, 'input_tokens': in_tok, 'output_tokens': out_tok}\n",
    "except Exception as e:\n",
    "    results['OpenAI'] = {'output': str(e), 'input_tokens': None, 'output_tokens': None}\n",
    "# Gemini\n",
    "try:\n",
    "    output, in_tok, out_tok = process_with_gemini(prompt)\n",
    "    results['Gemini'] = {'output': output, 'input_tokens': in_tok, 'output_tokens': out_tok}\n",
    "except Exception as e:\n",
    "    results['Gemini'] = {'output': str(e), 'input_tokens': None, 'output_tokens': None}\n",
    "# Anthropic\n",
    "try:\n",
    "    output, in_tok, out_tok = process_with_anthropic(prompt)\n",
    "    results['Anthropic'] = {'output': output, 'input_tokens': in_tok, 'output_tokens': out_tok}\n",
    "except Exception as e:\n",
    "    results['Anthropic'] = {'output': str(e), 'input_tokens': None, 'output_tokens': None}\n",
    "# Grok\n",
    "try:\n",
    "    output, in_tok, out_tok = process_with_grok(prompt)\n",
    "    results['Grok'] = {'output': output, 'input_tokens': in_tok, 'output_tokens': out_tok}\n",
    "except Exception as e:\n",
    "    results['Grok'] = {'output': str(e), 'input_tokens': None, 'output_tokens': None}\n",
    "import pandas as pd\n",
    "df = pd.DataFrame.from_dict(results, orient='index')\n",
    "display(df)\n",
    "\n",
    "# Optionally, print each output separately\n",
    "# for api, res in results.items():\n",
    "#     print(f'--- {api} ---')\n",
    "#     print(f'Output: {res[\"output\"]}\\nInput tokens: {res[\"input_tokens\"]}\\nOutput tokens: {res[\"output_tokens\"]}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "acb6d2c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results dumped to api_results_dump.txt\n"
     ]
    }
   ],
   "source": [
    "# Output all results and errors to a text file for easier reading\n",
    "with open('api_results.txt', 'w', encoding='utf-8') as f:\n",
    "    for api, res in results.items():\n",
    "        f.write(f'--- {api} ---\\n')\n",
    "        f.write(f'Output: {res[\"output\"]}\\nInput tokens: {res[\"input_tokens\"]}\\nOutput tokens: {res[\"output_tokens\"]}\\n\\n')\n",
    "print('Results dumped to api_results_dump.txt')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "all_llms",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
