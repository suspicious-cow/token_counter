{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "39284a43",
   "metadata": {},
   "source": [
    "# Token Counter Notebook\n",
    "\n",
    "This notebook allows you to input text, retrieve outputs, and calculate token counts using OpenAI, Gemini, Anthropic, and Grok APIs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "294f2b80",
   "metadata": {},
   "source": [
    "## 1. Install and Import Required Libraries\n",
    "\n",
    "Install necessary libraries such as `openai`, `anthropic`, and others. Import required modules like `os` and `json`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8e441ae2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required libraries (uncomment if running for the first time)\n",
    "# !pip install openai anthropic google-generativeai requests tiktoken\n",
    "\n",
    "import os\n",
    "import json\n",
    "import openai\n",
    "import anthropic\n",
    "from google import genai\n",
    "import requests\n",
    "import tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "311df612",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.jp-OutputArea-output { max-width: 100vw !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Make output cells take full width\n",
    "from IPython.core.display import HTML\n",
    "HTML(\"<style>.jp-OutputArea-output { max-width: 100vw !important; }</style>\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83c8ca06",
   "metadata": {},
   "source": [
    "## 2. Set Up API Keys\n",
    "\n",
    "Set up environment variables or directly define API keys for OpenAI, Gemini, Anthropic, and Grok."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "638367d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set your API keys here or load them from environment variables\n",
    "\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\", \"your-openai-api-key\")\n",
    "ANTHROPIC_API_KEY = os.getenv(\"ANTHROPIC_API_KEY\", \"your-anthropic-api-key\")\n",
    "GEMINI_API_KEY = os.getenv(\"GEMINI_API_KEY\", \"your-gemini-api-key\")\n",
    "GROK_API_KEY = os.getenv(\"GROK_API_KEY\", \"your-grok-api-key\")\n",
    "\n",
    "# Set API keys for respective libraries if possible\n",
    "openai.api_key = OPENAI_API_KEY"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b38fb37e",
   "metadata": {},
   "source": [
    "## 3. Enter Your System Prompt and User Prompt\n",
    "\n",
    "Define your input text in the variables below. These prompts will be sent to all four APIs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc3a11ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enter your user prompt here\n",
    "prompt = 'Explain the theory of relativity in one paragraph.'\n",
    "\n",
    "# Enter your system prompt here\n",
    "system_prompt = 'You are a helpful assistant.'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f1b2486",
   "metadata": {},
   "source": [
    "## 4. Define Helper Functions for Token Counting\n",
    "\n",
    "Write helper functions to calculate token counts for each API, ensuring compatibility with their respective tokenization methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1af6c4d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_openai_tokens(text, model=\"gpt-3.5-turbo\"):\n",
    "    enc = tiktoken.encoding_for_model(model)\n",
    "    return len(enc.encode(text))\n",
    "\n",
    "def count_gemini_tokens(text):\n",
    "    # Gemini uses SentencePiece tokenizer, but for simplicity, use whitespace split as a proxy\n",
    "    return len(text.split())\n",
    "\n",
    "def count_anthropic_tokens(text):\n",
    "    # Anthropic uses Claude tokenizer, which is similar to GPT-3's\n",
    "    enc = tiktoken.get_encoding(\"cl100k_base\")\n",
    "    return len(enc.encode(text))\n",
    "\n",
    "def count_grok_tokens(text):\n",
    "    # Grok's tokenizer is not public; use whitespace split as a proxy\n",
    "    return len(text.split())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea36191c",
   "metadata": {},
   "source": [
    "## 5. Process Text with OpenAI API\n",
    "\n",
    "Send the input text to the OpenAI API, retrieve the output, and calculate the token count using the helper function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df72b691",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_with_openai_chat(prompt, system_prompt, model=\"gpt-4o\"):\n",
    "    try:\n",
    "        from openai import OpenAI\n",
    "        client = OpenAI(api_key=OPENAI_API_KEY)\n",
    "        response = client.chat.completions.create(\n",
    "            model=model,\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": system_prompt},\n",
    "                {\"role\": \"user\", \"content\": prompt}\n",
    "            ]\n",
    "        )\n",
    "        output = response.choices[0].message.content\n",
    "        input_tokens = getattr(response.usage, 'prompt_tokens', None)\n",
    "        output_tokens = getattr(response.usage, 'completion_tokens', None)\n",
    "        return output, input_tokens, output_tokens\n",
    "    except Exception as e:\n",
    "        return f\"OpenAI ChatCompletions error: {str(e)}\", None, None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4374ab37",
   "metadata": {},
   "source": [
    "## 6. Process Text with Gemini API\n",
    "\n",
    "Send the input text to the Gemini API, retrieve the output, and calculate the token count using the helper function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bab4cccc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_with_gemini(prompt, system_prompt, model=\"gemini-2.0-flash\"):\n",
    "    try:\n",
    "        client = genai.Client(api_key=GEMINI_API_KEY)\n",
    "        response = client.models.generate_content(\n",
    "            model=model,\n",
    "            contents=[\n",
    "                {\"role\": \"system\", \"parts\": [system_prompt]},\n",
    "                {\"role\": \"user\", \"parts\": [prompt]}\n",
    "            ]\n",
    "        )\n",
    "        output = getattr(response, 'text', str(response))\n",
    "        usage = getattr(response, 'usage_metadata', None)\n",
    "        input_tokens = usage.prompt_token_count if usage and hasattr(usage, 'prompt_token_count') else None\n",
    "        output_tokens = usage.candidates_token_count if usage and hasattr(usage, 'candidates_token_count') else None\n",
    "        return output, input_tokens, output_tokens\n",
    "    except Exception as e:\n",
    "        return f\"Gemini error: {str(e)}\", None, None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89c02840",
   "metadata": {},
   "source": [
    "## 7. Process Text with Anthropic API\n",
    "\n",
    "Send the input text to the Anthropic API, retrieve the output, and calculate the token count using the helper function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb5d98dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_with_anthropic(prompt, system_prompt, model=\"claude-3-7-sonnet-20250219\"):\n",
    "    try:\n",
    "        import anthropic\n",
    "        client = anthropic.Anthropic(api_key=ANTHROPIC_API_KEY)\n",
    "        message = client.messages.create(\n",
    "            model=model,\n",
    "            max_tokens=1024,\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": system_prompt},\n",
    "                {\"role\": \"user\", \"content\": prompt}\n",
    "            ]\n",
    "        )\n",
    "        output = message.content[0].text if hasattr(message.content[0], 'text') else str(message.content[0])\n",
    "        input_tokens = getattr(message.usage, 'input_tokens', None)\n",
    "        output_tokens = getattr(message.usage, 'output_tokens', None)\n",
    "        return output, input_tokens, output_tokens\n",
    "    except Exception as e:\n",
    "        return f\"Anthropic error: {str(e)}\", None, None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da12f0c7",
   "metadata": {},
   "source": [
    "## 8. Process Text with Grok API\n",
    "\n",
    "Send the input text to the Grok API, retrieve the output, and calculate the token count using the helper function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "664cd833",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_with_grok(prompt, system_prompt, model=\"grok-3-beta\"):\n",
    "    try:\n",
    "        from openai import OpenAI\n",
    "        client = OpenAI(\n",
    "            api_key=GROK_API_KEY,\n",
    "            base_url=\"https://api.x.ai/v1\",\n",
    "        )\n",
    "        completion = client.chat.completions.create(\n",
    "            model=model,\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": system_prompt},\n",
    "                {\"role\": \"user\", \"content\": prompt}\n",
    "            ]\n",
    "        )\n",
    "        output = completion.choices[0].message.content\n",
    "        input_tokens = getattr(completion.usage, 'prompt_tokens', None)\n",
    "        output_tokens = getattr(completion.usage, 'completion_tokens', None)\n",
    "        return output, input_tokens, output_tokens\n",
    "    except Exception as e:\n",
    "        return f\"Grok error: {str(e)}\", None, None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f40aa52",
   "metadata": {},
   "source": [
    "## 9. Run All APIs and Compare Results\n",
    "\n",
    "This cell will send your prompt to OpenAI, Gemini, Anthropic, and Grok, then display their outputs and token counts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72c008eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>output</th>\n",
       "      <th>input_tokens</th>\n",
       "      <th>output_tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>OpenAI</th>\n",
       "      <td>The theory of relativity, developed by Albert ...</td>\n",
       "      <td>27</td>\n",
       "      <td>222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Gemini</th>\n",
       "      <td>Einstein's theory of relativity, comprising sp...</td>\n",
       "      <td>9</td>\n",
       "      <td>140</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Anthropic</th>\n",
       "      <td># Theory of Relativity in One Paragraph\\n\\nEin...</td>\n",
       "      <td>18</td>\n",
       "      <td>150</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Grok</th>\n",
       "      <td>The theory of relativity, developed by Albert ...</td>\n",
       "      <td>15</td>\n",
       "      <td>202</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                      output  input_tokens  \\\n",
       "OpenAI     The theory of relativity, developed by Albert ...            27   \n",
       "Gemini     Einstein's theory of relativity, comprising sp...             9   \n",
       "Anthropic  # Theory of Relativity in One Paragraph\\n\\nEin...            18   \n",
       "Grok       The theory of relativity, developed by Albert ...            15   \n",
       "\n",
       "           output_tokens  \n",
       "OpenAI               222  \n",
       "Gemini               140  \n",
       "Anthropic            150  \n",
       "Grok                 202  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "results = {}\n",
    "# OpenAI (Chat Completions API)\n",
    "try:\n",
    "    output, in_tok, out_tok = process_with_openai_chat(prompt, system_prompt)\n",
    "    results['OpenAI'] = {'output': output, 'input_tokens': in_tok, 'output_tokens': out_tok}\n",
    "except Exception as e:\n",
    "    results['OpenAI'] = {'output': str(e), 'input_tokens': None, 'output_tokens': None}\n",
    "# Gemini\n",
    "try:\n",
    "    output, in_tok, out_tok = process_with_gemini(prompt, system_prompt)\n",
    "    results['Gemini'] = {'output': output, 'input_tokens': in_tok, 'output_tokens': out_tok}\n",
    "except Exception as e:\n",
    "    results['Gemini'] = {'output': str(e), 'input_tokens': None, 'output_tokens': None}\n",
    "# Anthropic\n",
    "try:\n",
    "    output, in_tok, out_tok = process_with_anthropic(prompt, system_prompt)\n",
    "    results['Anthropic'] = {'output': output, 'input_tokens': in_tok, 'output_tokens': out_tok}\n",
    "except Exception as e:\n",
    "    results['Anthropic'] = {'output': str(e), 'input_tokens': None, 'output_tokens': None}\n",
    "# Grok\n",
    "try:\n",
    "    output, in_tok, out_tok = process_with_grok(prompt, system_prompt)\n",
    "    results['Grok'] = {'output': output, 'input_tokens': in_tok, 'output_tokens': out_tok}\n",
    "except Exception as e:\n",
    "    results['Grok'] = {'output': str(e), 'input_tokens': None, 'output_tokens': None}\n",
    "import pandas as pd\n",
    "df = pd.DataFrame.from_dict(results, orient='index')\n",
    "display(df)\n",
    "\n",
    "# Optionally, print each output separately\n",
    "# for api, res in results.items():\n",
    "#     print(f'--- {api} ---')\n",
    "#     print(f'Output: {res[\"output\"]}\\nInput tokens: {res[\"input_tokens\"]}\\nOutput tokens: {res[\"output_tokens\"]}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ab900ec",
   "metadata": {},
   "source": [
    "## 9. Put the results into an output file\n",
    "\n",
    "This cell will create a text-based output file that will display each LLMs outputs and token counts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "acb6d2c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results dumped to api_results_dump.txt\n"
     ]
    }
   ],
   "source": [
    "# Output all results and errors to a text file for easier reading\n",
    "with open('api_results.txt', 'w', encoding='utf-8') as f:\n",
    "    for api, res in results.items():\n",
    "        f.write(f'--- {api} ---\\n')\n",
    "        f.write(f'Output: {res[\"output\"]}\\nInput tokens: {res[\"input_tokens\"]}\\nOutput tokens: {res[\"output_tokens\"]}\\n\\n')\n",
    "print('Results dumped to api_results_dump.txt')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "all_llms",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
