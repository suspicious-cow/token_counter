{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "39284a43",
   "metadata": {},
   "source": [
    "# Token Counter Notebook\n",
    "\n",
    "This notebook allows you to input text, retrieve outputs, and calculate token counts using OpenAI, Gemini, Anthropic, and Grok APIs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "294f2b80",
   "metadata": {},
   "source": [
    "## 1. Install and Import Required Libraries\n",
    "\n",
    "Install necessary libraries such as `openai`, `anthropic`, and others. Import required modules like `os` and `json`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8e441ae2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required libraries (uncomment if running for the first time)\n",
    "# !pip install openai anthropic google-generativeai requests tiktoken\n",
    "\n",
    "import os\n",
    "import json\n",
    "import openai\n",
    "import anthropic\n",
    "from google import genai\n",
    "import requests\n",
    "import tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "311df612",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.jp-OutputArea-output { max-width: 100vw !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Make output cells take full width\n",
    "from IPython.core.display import HTML\n",
    "HTML(\"<style>.jp-OutputArea-output { max-width: 100vw !important; }</style>\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83c8ca06",
   "metadata": {},
   "source": [
    "## 2. Set Up API Keys\n",
    "\n",
    "Set up environment variables or directly define API keys for OpenAI, Gemini, Anthropic, and Grok."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "638367d2",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'google.genai' has no attribute 'configure'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 10\u001b[39m\n\u001b[32m      8\u001b[39m \u001b[38;5;66;03m# Set API keys for respective libraries\u001b[39;00m\n\u001b[32m      9\u001b[39m openai.api_key = OPENAI_API_KEY\n\u001b[32m---> \u001b[39m\u001b[32m10\u001b[39m \u001b[43mgenai\u001b[49m\u001b[43m.\u001b[49m\u001b[43mconfigure\u001b[49m(api_key=GEMINI_API_KEY)\n",
      "\u001b[31mAttributeError\u001b[39m: module 'google.genai' has no attribute 'configure'"
     ]
    }
   ],
   "source": [
    "# Set your API keys here or load them from environment variables\n",
    "\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\", \"your-openai-api-key\")\n",
    "ANTHROPIC_API_KEY = os.getenv(\"ANTHROPIC_API_KEY\", \"your-anthropic-api-key\")\n",
    "GEMINI_API_KEY = os.getenv(\"GEMINI_API_KEY\", \"your-gemini-api-key\")\n",
    "GROK_API_KEY = os.getenv(\"GROK_API_KEY\", \"your-grok-api-key\")\n",
    "\n",
    "# Set API keys for respective libraries if possible\n",
    "openai.api_key = OPENAI_API_KEY"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f1b2486",
   "metadata": {},
   "source": [
    "## 3. Define Helper Functions for Token Counting\n",
    "\n",
    "Write helper functions to calculate token counts for each API, ensuring compatibility with their respective tokenization methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1af6c4d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_openai_tokens(text, model=\"gpt-3.5-turbo\"):\n",
    "    enc = tiktoken.encoding_for_model(model)\n",
    "    return len(enc.encode(text))\n",
    "\n",
    "def count_gemini_tokens(text):\n",
    "    # Gemini uses SentencePiece tokenizer, but for simplicity, use whitespace split as a proxy\n",
    "    return len(text.split())\n",
    "\n",
    "def count_anthropic_tokens(text):\n",
    "    # Anthropic uses Claude tokenizer, which is similar to GPT-3's\n",
    "    enc = tiktoken.get_encoding(\"cl100k_base\")\n",
    "    return len(enc.encode(text))\n",
    "\n",
    "def count_grok_tokens(text):\n",
    "    # Grok's tokenizer is not public; use whitespace split as a proxy\n",
    "    return len(text.split())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea36191c",
   "metadata": {},
   "source": [
    "## 4. Process Text with OpenAI API\n",
    "\n",
    "Send the input text to the OpenAI API, retrieve the output, and calculate the token count using the helper function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df72b691",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_with_openai_response(prompt, model=\"gpt-4.1\"):\n",
    "    try:\n",
    "        from openai import OpenAI\n",
    "        client = OpenAI(api_key=OPENAI_API_KEY)\n",
    "        response = client.responses.create(\n",
    "            model=model,\n",
    "            input=prompt\n",
    "        )\n",
    "        output = getattr(response, 'output_text', str(response))\n",
    "        # Token usage may not be available; set to None\n",
    "        return output, None, None\n",
    "    except Exception as e:\n",
    "        return f\"OpenAI Responses API error: {str(e)}\", None, None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4374ab37",
   "metadata": {},
   "source": [
    "## 5. Process Text with Gemini API\n",
    "\n",
    "Send the input text to the Gemini API, retrieve the output, and calculate the token count using the helper function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bab4cccc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_with_gemini(prompt, model=\"gemini-2.0-flash\"):\n",
    "    try:\n",
    "        client = genai.Client(api_key=GEMINI_API_KEY)\n",
    "        response = client.models.generate_content(\n",
    "            model=model,\n",
    "            contents=[prompt],  # contents must be a list\n",
    "        )\n",
    "        output = getattr(response, 'text', str(response))\n",
    "        return output, None, None\n",
    "    except Exception as e:\n",
    "        return f\"Gemini error: {str(e)}\", None, None\n",
    "\n",
    "# Example usage:\n",
    "# output, input_tokens, output_tokens = process_with_gemini(prompt)\n",
    "# print(f\"Output: {output}\\nInput tokens: {input_tokens}\\nOutput tokens: {output_tokens}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89c02840",
   "metadata": {},
   "source": [
    "## 6. Process Text with Anthropic API\n",
    "\n",
    "Send the input text to the Anthropic API, retrieve the output, and calculate the token count using the helper function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb5d98dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_with_anthropic(prompt, model=\"claude-3-7-sonnet-20250219\"):\n",
    "    try:\n",
    "        import anthropic\n",
    "        client = anthropic.Anthropic(api_key=ANTHROPIC_API_KEY)\n",
    "        message = client.messages.create(\n",
    "            model=model,\n",
    "            max_tokens=1024,\n",
    "            messages=[{\"role\": \"user\", \"content\": prompt}]\n",
    "        )\n",
    "        output = message.content[0].text if hasattr(message.content[0], 'text') else str(message.content[0])\n",
    "        input_tokens = getattr(message.usage, 'input_tokens', None)\n",
    "        output_tokens = getattr(message.usage, 'output_tokens', None)\n",
    "        return output, input_tokens, output_tokens\n",
    "    except Exception as e:\n",
    "        return f\"Anthropic error: {str(e)}\", None, None\n",
    "\n",
    "# Example usage:\n",
    "# output, input_tokens, output_tokens = process_with_anthropic(prompt)\n",
    "# print(f\"Output: {output}\\nInput tokens: {input_tokens}\\nOutput tokens: {output_tokens}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da12f0c7",
   "metadata": {},
   "source": [
    "## 7. Process Text with Grok API\n",
    "\n",
    "Send the input text to the Grok API, retrieve the output, and calculate the token count using the helper function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "664cd833",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_with_grok(prompt, model=\"grok-3-beta\"):\n",
    "    try:\n",
    "        from openai import OpenAI\n",
    "        client = OpenAI(\n",
    "            api_key=GROK_API_KEY,\n",
    "            base_url=\"https://api.x.ai/v1\",\n",
    "        )\n",
    "        completion = client.chat.completions.create(\n",
    "            model=model,\n",
    "            messages=[\n",
    "                {\"role\": \"user\", \"content\": prompt}\n",
    "            ]\n",
    "        )\n",
    "        output = completion.choices[0].message.content\n",
    "        input_tokens = getattr(completion.usage, 'prompt_tokens', None)\n",
    "        output_tokens = getattr(completion.usage, 'completion_tokens', None)\n",
    "        return output, input_tokens, output_tokens\n",
    "    except Exception as e:\n",
    "        return f\"Grok error: {str(e)}\", None, None\n",
    "\n",
    "# Example usage:\n",
    "# output, input_tokens, output_tokens = process_with_grok(prompt)\n",
    "# print(f\"Output: {output}\\nInput tokens: {input_tokens}\\nOutput tokens: {output_tokens}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16b09ee1",
   "metadata": {},
   "source": [
    "## 8. Enter Your Prompt\n",
    "\n",
    "Define your input text in the variable below. This prompt will be sent to all four APIs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1af8226",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enter your prompt here\n",
    "prompt = 'Explain the theory of relativity in one paragraph.'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c9323d1",
   "metadata": {},
   "source": [
    "## 9. Run All APIs and Compare Results\n",
    "\n",
    "This cell will send your prompt to OpenAI, Gemini, Anthropic, and Grok, then display their outputs and token counts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72c008eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = {}\n",
    "# OpenAI (Responses API)\n",
    "try:\n",
    "    output, in_tok, out_tok = process_with_openai_response(prompt)\n",
    "    results['OpenAI'] = {'output': output, 'input_tokens': in_tok, 'output_tokens': out_tok}\n",
    "except Exception as e:\n",
    "    results['OpenAI'] = {'output': str(e), 'input_tokens': None, 'output_tokens': None}\n",
    "# Gemini\n",
    "try:\n",
    "    output, in_tok, out_tok = process_with_gemini(prompt)\n",
    "    results['Gemini'] = {'output': output, 'input_tokens': in_tok, 'output_tokens': out_tok}\n",
    "except Exception as e:\n",
    "    results['Gemini'] = {'output': str(e), 'input_tokens': None, 'output_tokens': None}\n",
    "# Anthropic\n",
    "try:\n",
    "    output, in_tok, out_tok = process_with_anthropic(prompt)\n",
    "    results['Anthropic'] = {'output': output, 'input_tokens': in_tok, 'output_tokens': out_tok}\n",
    "except Exception as e:\n",
    "    results['Anthropic'] = {'output': str(e), 'input_tokens': None, 'output_tokens': None}\n",
    "# Grok\n",
    "try:\n",
    "    output, in_tok, out_tok = process_with_grok(prompt)\n",
    "    results['Grok'] = {'output': output, 'input_tokens': in_tok, 'output_tokens': out_tok}\n",
    "except Exception as e:\n",
    "    results['Grok'] = {'output': str(e), 'input_tokens': None, 'output_tokens': None}\n",
    "import pandas as pd\n",
    "df = pd.DataFrame.from_dict(results, orient='index')\n",
    "display(df)\n",
    "\n",
    "# Optionally, print each output separately\n",
    "# for api, res in results.items():\n",
    "#     print(f'--- {api} ---')\n",
    "#     print(f'Output: {res[\"output\"]}\\nInput tokens: {res[\"input_tokens\"]}\\nOutput tokens: {res[\"output_tokens\"]}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acb6d2c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output all results and errors to a text file for easier reading\n",
    "with open('api_results.txt', 'w', encoding='utf-8') as f:\n",
    "    for api, res in results.items():\n",
    "        f.write(f'--- {api} ---\\n')\n",
    "        f.write(f'Output: {res[\"output\"]}\\nInput tokens: {res[\"input_tokens\"]}\\nOutput tokens: {res[\"output_tokens\"]}\\n\\n')\n",
    "print('Results dumped to api_results_dump.txt')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "all_llms",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
